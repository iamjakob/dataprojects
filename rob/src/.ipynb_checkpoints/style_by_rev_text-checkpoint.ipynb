{
 "metadata": {
  "name": "",
  "signature": "sha256:180c5082bf100800922f2471849742e6a61448c70ee867c0cc170272711b56b3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "import nltk.tokenize\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, TfidfTransformer\n",
      "from sklearn import cross_validation as c_v #,neighbors, datasets, grid_search\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.metrics import classification_report"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 280
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to db\n",
      "%load_ext sql\n",
      "%sql mysql://root:rootpwd@localhost/beerad\n",
      "\n",
      "# suppress row counts\n",
      "%config SqlMagic.feedback = False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The sql extension is already loaded. To reload it, use:\n",
        "  %reload_ext sql\n"
       ]
      }
     ],
     "prompt_number": 281
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get top 10 styles by review count\n",
      "styles = %sql \\\n",
      "select be.style_id, count(r.review) as b_ct \\\n",
      "from \\\n",
      "    beers be inner join reviews r \\\n",
      "        on be.id = r.beer_id \\\n",
      "    inner join styles s \\\n",
      "        on be.style_id = s.id \\\n",
      "group by be.style_id \\\n",
      "order by b_ct desc \\\n",
      "limit 1\n",
      "\n",
      "styles = styles.DataFrame()[\"style_id\"].values\n",
      "styles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 282,
       "text": [
        "array([116])"
       ]
      }
     ],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get 1000 reviews for each of the top 10 styles\n",
      "# need to go back and re-upload reviews to account for\n",
      "# \"Presentation\" error. These are actually the best reviews,\n",
      "# i.e. from The Bros, and used the delimiter \"Presentation:\" to start\n",
      "# the review text. Other 'power users' probably did the same\n",
      "revs = pd.DataFrame()\n",
      "for i in styles:\n",
      "    dt = %sql  \\\n",
      "        select r.review, be.style_id \\\n",
      "        from reviews r inner join beers be \\\n",
      "            on r.beer_id = be.id \\\n",
      "        where be.style_id = :i \\\n",
      "            and char_length(replace(trim(review),'\\n','')) > 30 \\\n",
      "        limit 10000\n",
      "        #and r.review not like \"%Presentation%\" \\\n",
      "        # only using 'pro' reviews lowered precision 25% points\n",
      "        #and lower(r.review) like '%presentation%' or lower(r.review) like '%p:%' \\\n",
      "    \n",
      "    revs = pd.concat([revs, dt.DataFrame()], ignore_index=True) #revs.append(dt.DataFrame())\n",
      "\n",
      "# original data source was not at all random\n",
      "nix = np.random.permutation(revs.index)\n",
      "revs = revs.reindex(index=nix, copy=False)\n",
      "\n",
      "X = pd.Series(revs[[\"review\"]].values.ravel())\n",
      "y = pd.Series(revs[[\"style_id\"]].values.ravel())\n",
      "\n",
      "# small look at table data\n",
      "print \"Review DF\"\n",
      "print revs[:1]\n",
      "print \"Series\"\n",
      "print X[0], y[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review DF\n",
        "                                                 review  style_id\n",
        "9115  Hazy, pale golden color, nice, flush white hea...       116\n",
        "Series\n",
        "Hazy, pale golden color, nice, flush white head, stalwart, slowly, but surely diminishing, ...Aroma:  spicy, citric, hoppy nose, pungent garpefrit, and peach pits, orange zest, and pineapple pulp, a little sour, and very tangy. very lively.  Those flavors return in the taste, but are accompanied by a huge sourness, an outsized citric twist on the palate, a little too unlean for me. Medium-bodied, with a big blast on first entry, a rounded fruity flavor, then thinning some, but not losing any of the bitterness. It rides long and zestily in the mouth, before eventually fading away. Long, fruity, bitter finish.  Very likable, this one, milder in the hop delivery than I like in my IPAs, but you can't please everyone. This ought to please plenty of folks just fine, and, heck, I like it a bit, too! 116\n"
       ]
      }
     ],
     "prompt_number": 283
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "default_tokenizer = TfidfVectorizer().build_tokenizer()\n",
      "stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n",
      "\n",
      "def non_num_tokenizer(text):\n",
      "    return default_tokenizer(re.sub(r'\\d', '', text))\n",
      "\n",
      "def tokenize_stem(text):\n",
      "    \"\"\"\n",
      "    use the default tokenizer from TfidfVectorizer, combined with the nltk SnowballStemmer.\n",
      "    \"\"\"\n",
      "    return map(stemmer.stem, non_num_tokenizer(text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 284
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vect_nb(max_features, ng_range, min_df, max_df, x, y):\n",
      "    stop_words = nltk.corpus.stopwords.words('english') + \\\n",
      "        ['ever','sure','want','review','got']\n",
      "    vec = TfidfVectorizer(max_features=max_features,\n",
      "                ngram_range=ng_range,\n",
      "                min_df=min_df,\n",
      "                max_df=max_df,\n",
      "                tokenizer=tokenize_stem,\n",
      "                stop_words=map(stemmer.stem, stop_words), \n",
      "                use_idf = True, sublinear_tf = False, \n",
      "                binary=False)\n",
      "    \n",
      "    dt = vec.fit_transform(x)\n",
      "    nb = MultinomialNB()\n",
      "    nb.fit(dt, y)\n",
      "    \n",
      "    return vec, nb"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 285
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Simple K-Fold cross validation. 5 folds.\n",
      "#cv = c_v.KFold(len(y), n_folds=5)#, indices=False)\n",
      "cv = c_v.StratifiedShuffleSplit(y, 3, test_size = 0.2, random_state = 0)\n",
      "#iterate through the training and test cross validation segments and\n",
      "#run the classifier on each one, aggregating the results into a list\n",
      "\n",
      "import pylab as pl\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "results = []\n",
      "res_rep = []\n",
      "max_features = 1000\n",
      "ngram_range = (1,1)\n",
      "min_df = 0.01\n",
      "max_df = .95\n",
      "for trainix, testix in cv:\n",
      "    # vectorize reviews and fit predictor\n",
      "    vec, nb = vect_nb(max_features, ngram_range, min_df, max_df, X[trainix].values, y[trainix].values)\n",
      "    \n",
      "    # predict class\n",
      "    pred = nb.predict(vec.transform(X[testix].values))\n",
      "    print classification_report(y[testix].values, pred)\n",
      "    cm = confusion_matrix(y[testix].values, pred)\n",
      "    pl.matshow(cm)\n",
      "    pl.title('Confusion matrix')\n",
      "    pl.colorbar()\n",
      "    pl.ylabel('True label')\n",
      "    pl.xlabel('Predicted label')\n",
      "    pl.show()\n",
      "    break\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "             precision    recall  f1-score   support\n",
        "\n",
        "        116       1.00      1.00      1.00      2000\n",
        "\n",
        "avg / total       1.00      1.00      1.00      2000\n",
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAO8AAADvCAYAAAAacIO5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE32qUZIkSZIkSZI0Kfwf21xFDi9ggKEAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0xfda0a10>"
       ]
      }
     ],
     "prompt_number": 292
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "idf = vec._tfidf.idf_\n",
      "w_lst = zip(vec.get_feature_names(), idf)\n",
      "w_lst.sort(key = lambda x: -x[1])\n",
      "print np.array(w_lst)[-50:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[u'much' u'2.66613325611']\n",
        " [u'note' u'2.6628318335']\n",
        " [u'finger' u'2.66217285489']\n",
        " [u'nose' u'2.63423989233']\n",
        " [u'leav' u'2.62214168683']\n",
        " [u'dri' u'2.57576552504']\n",
        " [u'smooth' u'2.54073671369']\n",
        " [u'golden' u'2.47689125647']\n",
        " [u'drink' u'2.43351096084']\n",
        " [u'pine' u'2.43089384341']\n",
        " [u'grapefruit' u'2.42049333364']\n",
        " [u'floral' u'2.40917634043']\n",
        " [u'great' u'2.35540529978']\n",
        " [u'littl' u'2.35298457722']\n",
        " [u'bit' u'2.33145779523']\n",
        " [u'slight' u'2.27801347946']\n",
        " [u'bottl' u'2.22685478364']\n",
        " [u'hoppi' u'2.21372734549']\n",
        " [u'amber' u'2.20368121663']\n",
        " [u'drinkabl' u'2.14582888839']\n",
        " [u'well' u'2.12984102591']\n",
        " [u'mouthfeel' u'2.10689546856']\n",
        " [u'medium' u'2.09973778119']\n",
        " [u'glass' u'2.09263096242']\n",
        " [u'light' u'2.08261789898']\n",
        " [u'like' u'2.05855549122']\n",
        " [u'balanc' u'2.0513767095']\n",
        " [u'orang' u'2.04745023649']\n",
        " [u'one' u'1.99201519256']\n",
        " [u'finish' u'1.98630185153']\n",
        " [u'bodi' u'1.95561143753']\n",
        " [u'sweet' u'1.90770378346']\n",
        " [u'flavor' u'1.88807125105']\n",
        " [u'aroma' u'1.88352509987']\n",
        " [u'carbon' u'1.85432159427']\n",
        " [u'white' u'1.8534409249']\n",
        " [u'smell' u'1.7977997021']\n",
        " [u'citrus' u'1.78319688028']\n",
        " [u'color' u'1.7655740845']\n",
        " [u'good' u'1.74246241694']\n",
        " [u'nice' u'1.65405145959']\n",
        " [u'lace' u'1.64901608149']\n",
        " [u'beer' u'1.5848662989']\n",
        " [u'malt' u'1.58039001569']\n",
        " [u'ipa' u'1.56531880844']\n",
        " [u'bitter' u'1.5117842967']\n",
        " [u'pour' u'1.50596307444']\n",
        " [u'tast' u'1.47494017843']\n",
        " [u'hop' u'1.1794011908']\n",
        " [u'head' u'1.14775530919']]\n"
       ]
      }
     ],
     "prompt_number": 293
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sql\n",
      "select * \n",
      "from styles\n",
      "where id in (84,97,116,140,159)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<table>\n",
        "    <tr>\n",
        "        <th>id</th>\n",
        "        <th>name</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>84</td>\n",
        "        <td>Russian Imperial Stout</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>97</td>\n",
        "        <td>American Pale Ale (APA)</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>116</td>\n",
        "        <td>American IPA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>140</td>\n",
        "        <td>American Double / Imperial IPA</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>159</td>\n",
        "        <td>American Porter</td>\n",
        "    </tr>\n",
        "</table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": [
        "[(84L, 'Russian Imperial Stout'),\n",
        " (97L, 'American Pale Ale (APA)'),\n",
        " (116L, 'American IPA'),\n",
        " (140L, 'American Double / Imperial IPA'),\n",
        " (159L, 'American Porter')]"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pipeline vectorizer\n",
      "def vectorizer(n_features):\n",
      "    from sklearn.pipeline import Pipeline\n",
      "    \n",
      "    hasher = HashingVectorizer(\n",
      "                n_features=n_features,\n",
      "                ngram_range=ng_range,\n",
      "                min_df=min_df,\n",
      "                max_df=max_df,\n",
      "                stop_words=map(stemmer.stem, nltk.corpus.stopwords.words('english')), \n",
      "                non_negative=False,\n",
      "                norm=None, binary=False)\n",
      "    \n",
      "    vec = Pipeline((\n",
      "            ('hash', hasher),\n",
      "            ('tf_idf', TfidfTransformer(use_idf = True, sublinear_tf = False, norm='l2')),\n",
      "        ))\n",
      "    \n",
      "    return vec\n",
      "\n",
      "#def classifier():\n",
      "#    from nltk.classify import SklearnClassifier\n",
      "#    from sklearn.feature_selection import SelectKBest\n",
      "#    pipeline = Pipeline((           \n",
      "#           ('hash', hasher),\n",
      "#            ('tf_idf', TfidfTransformer(use_idf = True, sublinear_tf = True, norm='l2')),\n",
      "#            ('chi2', SelectKBest(chi2, k=1000))\n",
      "#            ('nb', MultinomialNB())\n",
      "#        ))\n",
      "#    return SklearnClassifier(pipeline)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.corpus.stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 270,
       "text": [
        "['i',\n",
        " 'me',\n",
        " 'my',\n",
        " 'myself',\n",
        " 'we',\n",
        " 'our',\n",
        " 'ours',\n",
        " 'ourselves',\n",
        " 'you',\n",
        " 'your',\n",
        " 'yours',\n",
        " 'yourself',\n",
        " 'yourselves',\n",
        " 'he',\n",
        " 'him',\n",
        " 'his',\n",
        " 'himself',\n",
        " 'she',\n",
        " 'her',\n",
        " 'hers',\n",
        " 'herself',\n",
        " 'it',\n",
        " 'its',\n",
        " 'itself',\n",
        " 'they',\n",
        " 'them',\n",
        " 'their',\n",
        " 'theirs',\n",
        " 'themselves',\n",
        " 'what',\n",
        " 'which',\n",
        " 'who',\n",
        " 'whom',\n",
        " 'this',\n",
        " 'that',\n",
        " 'these',\n",
        " 'those',\n",
        " 'am',\n",
        " 'is',\n",
        " 'are',\n",
        " 'was',\n",
        " 'were',\n",
        " 'be',\n",
        " 'been',\n",
        " 'being',\n",
        " 'have',\n",
        " 'has',\n",
        " 'had',\n",
        " 'having',\n",
        " 'do',\n",
        " 'does',\n",
        " 'did',\n",
        " 'doing',\n",
        " 'a',\n",
        " 'an',\n",
        " 'the',\n",
        " 'and',\n",
        " 'but',\n",
        " 'if',\n",
        " 'or',\n",
        " 'because',\n",
        " 'as',\n",
        " 'until',\n",
        " 'while',\n",
        " 'of',\n",
        " 'at',\n",
        " 'by',\n",
        " 'for',\n",
        " 'with',\n",
        " 'about',\n",
        " 'against',\n",
        " 'between',\n",
        " 'into',\n",
        " 'through',\n",
        " 'during',\n",
        " 'before',\n",
        " 'after',\n",
        " 'above',\n",
        " 'below',\n",
        " 'to',\n",
        " 'from',\n",
        " 'up',\n",
        " 'down',\n",
        " 'in',\n",
        " 'out',\n",
        " 'on',\n",
        " 'off',\n",
        " 'over',\n",
        " 'under',\n",
        " 'again',\n",
        " 'further',\n",
        " 'then',\n",
        " 'once',\n",
        " 'here',\n",
        " 'there',\n",
        " 'when',\n",
        " 'where',\n",
        " 'why',\n",
        " 'how',\n",
        " 'all',\n",
        " 'any',\n",
        " 'both',\n",
        " 'each',\n",
        " 'few',\n",
        " 'more',\n",
        " 'most',\n",
        " 'other',\n",
        " 'some',\n",
        " 'such',\n",
        " 'no',\n",
        " 'nor',\n",
        " 'not',\n",
        " 'only',\n",
        " 'own',\n",
        " 'same',\n",
        " 'so',\n",
        " 'than',\n",
        " 'too',\n",
        " 'very',\n",
        " 's',\n",
        " 't',\n",
        " 'can',\n",
        " 'will',\n",
        " 'just',\n",
        " 'don',\n",
        " 'should',\n",
        " 'now']"
       ]
      }
     ],
     "prompt_number": 270
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec.transform(X[testix[0]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 296,
       "text": [
        "<451x723 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 0 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 296
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}